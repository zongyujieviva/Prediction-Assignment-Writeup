<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="YZong" />


<title>Machine_Learning_Project.R</title>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type="text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Machine_Learning_Project.R</h1>
<h4 class="author"><em>YZong</em></h4>
<h4 class="date"><em>Tue Dec 26 17:18:45 2017</em></h4>

</div>


<pre class="r"><code>library(AppliedPredictiveModeling)
library(ElemStatLearn)</code></pre>
<pre><code>## Warning: package 'ElemStatLearn' was built under R version 3.4.3</code></pre>
<pre class="r"><code>library(lubridate)</code></pre>
<pre><code>## 
## Attaching package: 'lubridate'</code></pre>
<pre><code>## The following object is masked from 'package:base':
## 
##     date</code></pre>
<pre class="r"><code>library(ggplot2)
library(caret)</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre class="r"><code>library(gbm)</code></pre>
<pre><code>## Warning: package 'gbm' was built under R version 3.4.3</code></pre>
<pre><code>## Loading required package: survival</code></pre>
<pre><code>## 
## Attaching package: 'survival'</code></pre>
<pre><code>## The following object is masked from 'package:caret':
## 
##     cluster</code></pre>
<pre><code>## Loading required package: splines</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<pre><code>## Loaded gbm 2.1.3</code></pre>
<pre class="r"><code>### Data import
training &lt;- read.csv(&quot;C:/Users/yzong/Documents/Coursera/testdir/pml-training.csv&quot;,na.strings = c(&quot;NA&quot;, &quot;#DIV/0!&quot;, &quot;&quot;))
testing &lt;- read.csv(&quot;C:/Users/yzong/Documents/Coursera/testdir/pml-testing.csv&quot;,na.strings = c(&quot;NA&quot;, &quot;#DIV/0!&quot;, &quot;&quot;))

### Preliminary check
dim(training)</code></pre>
<pre><code>## [1] 19622   160</code></pre>
<pre class="r"><code>dim(testing)</code></pre>
<pre><code>## [1]  20 160</code></pre>
<pre class="r"><code>table(training$classe)</code></pre>
<pre><code>## 
##    A    B    C    D    E 
## 5580 3797 3422 3216 3607</code></pre>
<pre class="r"><code>prop.table(table(training$user_name))</code></pre>
<pre><code>## 
##    adelmo  carlitos   charles    eurico    jeremy     pedro 
## 0.1983488 0.1585975 0.1802059 0.1564570 0.1733768 0.1330140</code></pre>
<pre class="r"><code>#### Data Cleaning
#### Remove those variables that has missing values
training &lt;- training[, colSums(is.na(training)) == 0]
testing &lt;- testing[, colSums(is.na(testing)) == 0] 

### Remove unrelated variables
classe &lt;- training$classe
trainRemove &lt;- grepl(&quot;^X|timestamp|window&quot;, names(training))
training &lt;- training[, !trainRemove]
training &lt;- training[, sapply(training, is.numeric)]
training$classe&lt;- classe
testRemove &lt;- grepl(&quot;^X|timestamp|window&quot;, names(testing))
testing &lt;- testing[, !testRemove]
testing &lt;- testing[, sapply(testing, is.numeric)]

### Now both training and testing data constrain to 53 variables
dim(training)</code></pre>
<pre><code>## [1] 19622    53</code></pre>
<pre class="r"><code>dim(testing)</code></pre>
<pre><code>## [1] 20 53</code></pre>
<pre class="r"><code>### Create training and testing data within the big training file
set.seed(3433)
inTrain = createDataPartition(training$classe, p = 0.7)[[1]]
mytrain = training[inTrain,]
mytest = training[-inTrain,]

### Try two models by using method = &quot;rf&quot; and &quot;rpart&quot;
mytrain$classe &lt;- as.factor(mytrain$classe)
mytest$classe &lt;- as.factor(mytest$classe)
set.seed(12345)
control &lt;- trainControl(method=&quot;cv&quot;, 5)
F1 &lt;- train(classe ~ ., method = &quot;rf&quot;, data = mytrain,trControl=control, importance=TRUE, ntree=100)</code></pre>
<pre><code>## Warning: package 'randomForest' was built under R version 3.4.3</code></pre>
<pre><code>## randomForest 4.6-12</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: 'randomForest'</code></pre>
<pre><code>## The following object is masked from 'package:ggplot2':
## 
##     margin</code></pre>
<pre class="r"><code>F2 &lt;- train(classe ~ ., method = &quot;rpart&quot;, data = mytrain)
F1</code></pre>
<pre><code>## Random Forest 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 10990, 10991, 10989, 10990, 10988 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.9911914  0.9888562
##   27    0.9903909  0.9878441
##   52    0.9847129  0.9806594
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<pre class="r"><code>### Model 1 return the accuracy = 99.12% and out-of-sample error = 0.88%
<pre class="r"><code>### Then we apply the model fit to mytest and see the accuracy.
<pre class="r"><code>### Model 1 still has 99%; Model 2 is still below 50%
pred1 &lt;- predict(F1,mytest)
pred2 &lt;- predict(F2,mytest)
confusionMatrix(pred1, mytest$classe)$overall[1]</code></pre>
<pre><code>##  Accuracy 
## 0.9896347</code></pre>
<pre class="r"><code>confusionMatrix(pred2, mytest$classe)$overall[1]</code></pre>
<pre><code>##  Accuracy 
## 0.4898895</code></pre>
<pre class="r"><code>### use model 1 to fit the test data to predict the results
result &lt;- predict(F1, testing[, -length(names(testing))])
result</code></pre>
<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E</code></pre>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
