
<body>


<div class="container-fluid main-container">

<!-- tabsets -->

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Machine_Learning_Project.R</h1>
<h4 class="author"><em>YZong</em></h4>
<h4 class="date"><em>Tue Dec 26 17:18:45 2017</em></h4>

</div>


<pre class="r"><code>library(AppliedPredictiveModeling)
library(ElemStatLearn)</code></pre>
<pre><code>## Warning: package 'ElemStatLearn' was built under R version 3.4.3</code></pre>
<pre class="r"><code>library(lubridate)</code></pre>
<pre><code>## 
## Attaching package: 'lubridate'</code></pre>
<pre><code>## The following object is masked from 'package:base':
## 
##     date</code></pre>
<pre class="r"><code>library(ggplot2)
library(caret)</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre class="r"><code>library(gbm)</code></pre>
<pre><code>## Warning: package 'gbm' was built under R version 3.4.3</code></pre>
<pre><code>## Loading required package: survival</code></pre>
<pre><code>## 
## Attaching package: 'survival'</code></pre>
<pre><code>## The following object is masked from 'package:caret':
## 
##     cluster</code></pre>
<pre><code>## Loading required package: splines</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<pre><code>## Loaded gbm 2.1.3</code></pre>
<pre class="r"><code>### Data import
training &lt;- read.csv(&quot;C:/Users/yzong/Documents/Coursera/testdir/pml-training.csv&quot;,na.strings = c(&quot;NA&quot;, &quot;#DIV/0!&quot;, &quot;&quot;))
testing &lt;- read.csv(&quot;C:/Users/yzong/Documents/Coursera/testdir/pml-testing.csv&quot;,na.strings = c(&quot;NA&quot;, &quot;#DIV/0!&quot;, &quot;&quot;))

### Preliminary check
dim(training)</code></pre>
<pre><code>## [1] 19622   160</code></pre>
<pre class="r"><code>dim(testing)</code></pre>
<pre><code>## [1]  20 160</code></pre>
<pre class="r"><code>table(training$classe)</code></pre>
<pre><code>## 
##    A    B    C    D    E 
## 5580 3797 3422 3216 3607</code></pre>
<pre class="r"><code>prop.table(table(training$user_name))</code></pre>
<pre><code>## 
##    adelmo  carlitos   charles    eurico    jeremy     pedro 
## 0.1983488 0.1585975 0.1802059 0.1564570 0.1733768 0.1330140</code></pre>
<pre class="r"><code>#### Data Cleaning
#### Remove those variables that has missing values
training &lt;- training[, colSums(is.na(training)) == 0]
testing &lt;- testing[, colSums(is.na(testing)) == 0] 

### Remove unrelated variables
classe &lt;- training$classe
trainRemove &lt;- grepl(&quot;^X|timestamp|window&quot;, names(training))
training &lt;- training[, !trainRemove]
training &lt;- training[, sapply(training, is.numeric)]
training$classe&lt;- classe
testRemove &lt;- grepl(&quot;^X|timestamp|window&quot;, names(testing))
testing &lt;- testing[, !testRemove]
testing &lt;- testing[, sapply(testing, is.numeric)]

### Now both training and testing data constrain to 53 variables
dim(training)</code></pre>
<pre><code>## [1] 19622    53</code></pre>
<pre class="r"><code>dim(testing)</code></pre>
<pre><code>## [1] 20 53</code></pre>
<pre class="r"><code>### Create training and testing data within the big training file
set.seed(3433)
inTrain = createDataPartition(training$classe, p = 0.7)[[1]]
mytrain = training[inTrain,]
mytest = training[-inTrain,]

### Try two models by using method = &quot;rf&quot; and &quot;rpart&quot;
mytrain$classe &lt;- as.factor(mytrain$classe)
mytest$classe &lt;- as.factor(mytest$classe)
set.seed(12345)
control &lt;- trainControl(method=&quot;cv&quot;, 5)
F1 &lt;- train(classe ~ ., method = &quot;rf&quot;, data = mytrain,trControl=control, importance=TRUE, ntree=100)</code></pre>
<pre><code>## Warning: package 'randomForest' was built under R version 3.4.3</code></pre>
<pre><code>## randomForest 4.6-12</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: 'randomForest'</code></pre>
<pre><code>## The following object is masked from 'package:ggplot2':
## 
##     margin</code></pre>
<pre class="r"><code>F2 &lt;- train(classe ~ ., method = &quot;rpart&quot;, data = mytrain)
F1</code></pre>
<pre><code>## Random Forest 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 10990, 10991, 10989, 10990, 10988 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.9911914  0.9888562
##   27    0.9903909  0.9878441
##   52    0.9847129  0.9806594
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<pre class="r"><code>### Model 1 return the accuracy = 99.12% and out-of-sample error = 0.88%
### Then we apply the model fit to mytest and see the accuracy.
### Model 1 still has 99%; Model 2 is still below 50%
pred1 &lt;- predict(F1,mytest)
pred2 &lt;- predict(F2,mytest)
confusionMatrix(pred1, mytest$classe)$overall[1]</code></pre>
<pre><code>##  Accuracy 
## 0.9896347</code></pre>
<pre class="r"><code>confusionMatrix(pred2, mytest$classe)$overall[1]</code></pre>
<pre><code>##  Accuracy 
## 0.4898895</code></pre>
<pre class="r"><code>### use model 1 to fit the test data to predict the results
result &lt;- predict(F1, testing[, -length(names(testing))])
result</code></pre>
<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E</code></pre>




</div>


</body>
</html>
